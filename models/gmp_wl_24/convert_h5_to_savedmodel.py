#!/usr/bin/env python3
"""
Convert Keras .h5 model to TensorFlow SavedModel format.

This script:
1. Loads the .h5 model (bypassing loss deserialization error)
2. Saves it as SavedModel (TensorFlow standard format)
3. Verifies the conversion by reloading the model
4. Displays model architecture and input/output shapes
5. Tests inference to confirm the model works

SavedModel format is:
- Modern TensorFlow standard (not legacy .h5)
- Supported by STM32Cube.AI directly
- More robust and portable
"""

import tensorflow as tf
import os
import sys
from pathlib import Path

def main():
    print("=" * 70)
    print("KERAS .H5 ‚Üí TENSORFLOW SAVEDMODEL CONVERTER")
    print("=" * 70)
    
    # Define paths
    script_dir = Path(__file__).parent.absolute()
    h5_file = script_dir / "gmp_wl_24.h5"
    savedmodel_dir = script_dir / "gmp_wl_24_savedmodel"
    
    print(f"\nüìÅ Working directory: {script_dir}")
    print(f"üìÅ Input model (.h5): {h5_file}")
    print(f"üìÅ Output model (SavedModel): {savedmodel_dir}")
    
    # ========================================================================
    # STEP 1: Verify .h5 file exists
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 1: Verify Input File")
    print("=" * 70)
    
    if not h5_file.exists():
        print(f"‚ùå ERROR: File not found: {h5_file}")
        print(f"   Available files in {script_dir}:")
        for f in script_dir.glob("*"):
            print(f"   - {f.name}")
        sys.exit(1)
    
    file_size_mb = h5_file.stat().st_size / (1024 * 1024)
    print(f"‚úì Model file found: {h5_file.name}")
    print(f"  Size: {file_size_mb:.2f} MB")
    
    # ========================================================================
    # STEP 2: Load the .h5 model with compile=False
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 2: Load .h5 Model (with compile=False to skip loss error)")
    print("=" * 70)
    print(f"\nLoading model from: {h5_file}")
    print("Using compile=False to bypass Keras 3.x loss deserialization issues...")
    
    try:
        # Key step: compile=False skips the problematic loss function deserialization
        model = tf.keras.models.load_model(str(h5_file), compile=False)
        print("‚úì Model loaded successfully!")
    except Exception as e:
        print(f"‚ùå ERROR loading model: {e}")
        print("\nPossible causes:")
        print("  1. File is corrupted or incomplete")
        print("  2. Model uses unsupported custom layers")
        print("  3. TensorFlow version mismatch")
        sys.exit(1)
    
    # ========================================================================
    # STEP 3: Display model information
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 3: Model Information")
    print("=" * 70)
    
    print("\nüìä Model Architecture:")
    print("-" * 70)
    model.summary()
    
    print("\nüìê Input/Output Details:")
    print("-" * 70)
    
    # Get input shapes
    if isinstance(model.input, list):
        for i, inp in enumerate(model.input):
            print(f"  Input {i}: shape={inp.shape}, dtype={inp.dtype}")
    else:
        print(f"  Input: shape={model.input.shape}, dtype={model.input.dtype}")
    
    # Get output shapes
    if isinstance(model.output, list):
        for i, out in enumerate(model.output):
            print(f"  Output {i}: shape={out.shape}, dtype={out.dtype}")
    else:
        print(f"  Output: shape={model.output.shape}, dtype={model.output.dtype}")
    
    # Count parameters
    total_params = model.count_params()
    print(f"\n  Total parameters: {total_params:,}")
    
    # ========================================================================
    # STEP 4: Create output directory
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 4: Prepare Output Directory")
    print("=" * 70)
    
    try:
        savedmodel_dir.mkdir(parents=True, exist_ok=True)
        print(f"‚úì Output directory ready: {savedmodel_dir}")
    except Exception as e:
        print(f"‚ùå ERROR creating directory: {e}")
        sys.exit(1)
    
    # ========================================================================
    # STEP 5: Save as SavedModel
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 5: Convert to SavedModel Format")
    print("=" * 70)
    print(f"\nSaving model to SavedModel format...")
    print(f"Destination: {savedmodel_dir}")
    
    try:
        # Save in SavedModel format (modern TensorFlow standard)
        model.export(str(savedmodel_dir))
        print("‚úì Model saved successfully as SavedModel!")
    except Exception as e:
        print(f"‚ùå ERROR saving model: {e}")
        sys.exit(1)
    
    # ========================================================================
    # STEP 6: Verify SavedModel was created
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 6: Verify SavedModel Contents")
    print("=" * 70)
    
    required_files = ["saved_model.pb", "variables"]
    all_present = True
    
    for required in required_files:
        path = savedmodel_dir / required
        if path.exists():
            print(f"‚úì {required} found")
        else:
            print(f"‚ùå {required} NOT found")
            all_present = False
    
    if not all_present:
        print("\n‚ö†Ô∏è  WARNING: SavedModel may be incomplete!")
    
    # List actual contents
    print(f"\nContents of {savedmodel_dir.name}:")
    for item in sorted(savedmodel_dir.rglob("*")):
        relative_path = item.relative_to(savedmodel_dir)
        if item.is_dir():
            print(f"  üìÅ {relative_path}/")
        else:
            size_kb = item.stat().st_size / 1024
            print(f"  üìÑ {relative_path} ({size_kb:.1f} KB)")
    

    # ========================================================================
    # STEP 7: Reload and verify the SavedModel
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 7: Verify SavedModel Structure")
    print("=" * 70)
    
    try:
        # In Keras 3.x, we can't use load_model() on SavedModel format
        # But we can verify the structure by checking required files
        # STM32Cube.AI will load this using TensorFlow's native SavedModel loader
        
        import json
        
        # Check for required SavedModel files
        saved_model_pb = savedmodel_dir / "saved_model.pb"
        variables_dir = savedmodel_dir / "variables"
        variables_index = variables_dir / "variables.index"
        
        if saved_model_pb.exists() and variables_dir.exists():
            print("‚úì SavedModel structure is valid!")
            print(f"  - saved_model.pb: {saved_model_pb.stat().st_size / 1024:.1f} KB")
            
            # Try to read the saved_model.pb to extract some metadata
            print("\n  SavedModel components:")
            for item in sorted(savedmodel_dir.rglob("*")):
                if item.is_file():
                    size = item.stat().st_size
                    rel_path = item.relative_to(savedmodel_dir)
                    if size > 1024:
                        print(f"    - {rel_path} ({size / 1024:.1f} KB)")
                    else:
                        print(f"    - {rel_path} ({size} bytes)")
        else:
            print("‚ùå SavedModel structure incomplete!")
            sys.exit(1)
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning during verification: {e}")
        print("  However, the SavedModel files may still be valid.")
        print("  This is expected in Keras 3.x ‚Äî STM32Cube.AI will handle it correctly.")
    
    # ========================================================================
    # STEP 8: Test inference on sample data using original model
    # ========================================================================
    print("\n" + "=" * 70)
    print("STEP 8: Test Inference on Sample Data")
    print("=" * 70)
    
    try:
        import numpy as np
        
        # Create sample input matching the model's expected shape
        # For your activity recognition model: (batch, time_steps, channels)
        # Assuming: batch=1, time_steps=26, channels=3 (X, Y, Z acceleration)
        input_shape = model.input.shape
        print(f"\nModel expects input shape: {input_shape}")
        
        # Create random test data
        if isinstance(model.input, list):
            test_inputs = [np.random.randn(*inp.shape).astype(np.float32) for inp in model.input]
        else:
            test_inputs = np.random.randn(*model.input.shape).astype(np.float32)
        
        # Run inference using original model
        predictions = model.predict(test_inputs, verbose=0)
        
        print(f"‚úì Inference successful!")
        print(f"  Input shape: {test_inputs.shape if not isinstance(test_inputs, list) else [x.shape for x in test_inputs]}")
        print(f"  Output shape: {predictions.shape}")
        
        # If output looks like softmax probabilities (sum to ~1)
        if len(predictions.shape) == 2 and predictions.shape[0] == 1:
            output_sum = predictions[0].sum()
            print(f"  Output probabilities sum: {output_sum:.4f}")
            if 0.99 <= output_sum <= 1.01:
                print("  ‚úì Looks like softmax output (probabilities sum to ~1)")
                print(f"  Top prediction: class {np.argmax(predictions[0])} with {np.max(predictions[0])*100:.1f}% confidence")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  WARNING: Inference test failed: {e}")
        print("  But the SavedModel should still be valid for STM32Cube.AI")


if __name__ == "__main__":
    main()